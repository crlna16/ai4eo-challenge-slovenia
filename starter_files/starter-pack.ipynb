{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI4EO - Feature enhancement for land management\n",
    "\n",
    "## Get started with the challenge\n",
    "\n",
    "This notebook will get you started with downloading, exploring and analysing the input and output data of the challenge.\n",
    "\n",
    "The aim of this challenge is to create AI systems that can exploit the temporal information of Sentinel-2 images into an enhanced spatial resolution. \n",
    "\n",
    "Your task will be to estimate a _cultivated land_ map at **2.5m** spatial resolution given as input a _Sentinel-2 time-series_ at **10m** spatial resolution, therefore resulting in a `4x` spatial resolution improvement. \n",
    "\n",
    "This notebook showcases how to download and process the data using [eo-learn](https://eo-learn.readthedocs.io/en/latest/index.html), a Python library specifically designed to deal with Earth Observation data. The data will be stored into `EOPatches` as `numpy` arrays and `geopandas` dataframes to facilitate processing operations. However, you can use any other Python tool of preference to process the provided data.\n",
    "\n",
    "The notebook also showcases how to generate a valid submission file.\n",
    "\n",
    "As per challenge rules, the following applies:\n",
    " * no data source other than the ones provided can be used to produce your outputs;\n",
    " * pre-trained models are not allowed.\n",
    "\n",
    "**NOTE**: Code for the winning solutions will be executed to reproduce your winning submission and reviewed to ensure rules have been followed. Failure to reproduce the submission will result in its disqualification. Make sure therefore to set your random seeds and initialisations where needed.\n",
    "\n",
    "The content of the notebook is as follows:\n",
    "\n",
    "0. [Requirements](#requirements)\n",
    "\n",
    "1. [Data overview](#data-overview)\n",
    "\n",
    "   1.1. [Area of Interest](#aoi)\n",
    "\n",
    "   1.2. [Sentinel-2 time-series](#sentinel-2)\n",
    "     \n",
    "   1.3. [Reference polygons](#reference)\n",
    "   \n",
    "   1.4. [Train data download](#train-data)\n",
    "   \n",
    "   1.5. [Test data download](#test-data)\n",
    "   \n",
    "   1.6. [Files check](#files-check)\n",
    "    \n",
    "2. [Validation metric](#validation-metric)\n",
    "    \n",
    "3. [Data processing](#data-processing)\n",
    "\n",
    "4. [Submission example](#submission-example)\n",
    "\n",
    "   4.1 [Prepare a submission](#prepare-a-submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook related\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in modules\n",
    "import os\n",
    "import json\n",
    "import datetime as dt\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Basics of Python data handling and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Module for GeoDB\n",
    "from xcube_geodb.core.geodb import GeoDBClient\n",
    "\n",
    "# Imports from eo-learn and sentinelhub-py\n",
    "from sentinelhub import CRS, BBox, SHConfig, DataCollection\n",
    "\n",
    "from eolearn.core import (FeatureType,\n",
    "                          EOPatch, \n",
    "                          EOTask, \n",
    "                          LinearWorkflow, \n",
    "                          EOExecutor, \n",
    "                          LoadTask,\n",
    "                          SaveTask)\n",
    "from eolearn.io import GeoDBVectorImportTask, SentinelHubInputTask\n",
    "from eolearn.geometry import VectorToRaster\n",
    "\n",
    "# Visualisation utilities from utils.py\n",
    "from utils import get_extent, md5_encode_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Requirements  <a name=\"requirements\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to download the training data through the provided APIs you would need to set-up an account for Sentinel Hub and GeoDB. Check the following options.\n",
    "\n",
    "#### If you are using the Euro Data Cube (EDC) resources\n",
    "\n",
    "The accounts have been already created and configured for you, and what you need is only to retrieve the SentinelHub credentials from your dashboard and paste them below (from your EDC dashboard click on **EDC Sentinel Hub** in the _My API service subscriptions_, then click on **SHOW** credential in _API Access_ section).  \n",
    "\n",
    "The GeoDB account is already set-up and you don't need to retrieve the credentials.\n",
    "\n",
    "Alternatively to the dashboard, you can retrieve all credentials from the `.env` file in your home directory.\n",
    "\n",
    "#### If you are using your own resources\n",
    "\n",
    "You need the following accounts:\n",
    "\n",
    " * [Sentinel Hub](https://docs.sentinel-hub.com/api/latest/), to download the Sentinel-2 time-series. What you need is to [create a free account](http://www.sentinel-hub.com/create_account), register an OAuth client from [your dashboard](https://apps.sentinel-hub.com/dashboard/#/account/settings) by clicking on the `Create a new OAuth client` button, set the `Client grant type` to `Client Credentials` and save the `secret` value. Paste below the `client_id` and `secret_client_id` and you are ready to go;\n",
    " * [GeoDB](https://eurodatacube.com/marketplace/services/edc_geodb), to download polygons defining the AOI and the reference land cover polygons. [Create a trial account](https://eurodatacube.com/register), and retrieve the `client_id` and `client_secret` tokens from your dashboard. \n",
    " \n",
    "#### If you cannot use the APIs\n",
    "\n",
    "Check the challenge description for alternative options to retrieve the training data as a single compressed file. Beware that the resulting dataset is approximately 11GB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_config = SHConfig()\n",
    "\n",
    "sh_config.sh_client_id = '<your Sentinel Hub OAuth client ID>'\n",
    "sh_config.sh_client_secret = '<your Sentinel Hub OAuth client secret>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodb_client_id = '<your GeoDB client ID>'\n",
    "geodb_client_secret = '<your GeoDB client secret>'\n",
    "\n",
    "# uncomment below if running from EDC\n",
    "# client = GeoDBClient()\n",
    "\n",
    "# uncomment below if running from your own resources\n",
    "client = GeoDBClient(server_url='https://xcube-geodb.brockmann-consult.de', \n",
    "                      auth_aud='https://xcube-users.brockmann-consult.de/api/v2', \n",
    "                      client_id=geodb_client_id, client_secret=geodb_client_secret)\n",
    "\n",
    "client.whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Overview <a name=\"data-overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section gives an overview of all the necessary data to train your model and how to download them. \n",
    "\n",
    "In particular, the data sources to download are the following:\n",
    "\n",
    " * a list of bounding boxes defining the training area-of-interest (AOI);\n",
    " * the Sentinel-2 L2A time-series for a 6 months period starting 1st March 2019;\n",
    " * the reference polygons defining the cultivated and non-cultivated land labels. \n",
    " \n",
    "After downloading the bounding boxes defining the AOI, we will download the images and reference data for an example bounding box. After this, we will set a workflow to download all the required training data.\n",
    "\n",
    "**NOTE**: The download parameters **CANNOT** be modified! Otherwise you will work with the wrong input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS\n",
    "GEODB_DATABASE = 'geodb_0e5d743f-2134-4561-8946-a073b039176f'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Area of Interest <a name=\"aoi\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AOI chosen for this challenge is the [Republic of Slovenia](https://en.wikipedia.org/wiki/Slovenia) and its neighbouring countries. \n",
    "\n",
    "The AOI is split into 125 bounding boxes across the entire area, data for 100 bounding boxes to be used for training, and 25 for testing. The following code retrieves the bounding boxes for training from the GeoDB database.\n",
    "\n",
    "`bboxes` is a `geopandas` GeoDataFrame holding the geometry information and a unique index identifier of the bounding box, i.e. `eop_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = client.get_collection('ai4eo_bboxes', database=GEODB_DATABASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The bounding boxes and all data sources used are in the UTM zone 33N, i.e. `epsg:32633`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = gpd.read_file('metadata/svn-border.geojson')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "country.plot(ax=ax, color='xkcd:very light green')\n",
    "bboxes.boundary.plot(ax=ax, color='xkcd:blue', label='Train')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylabel(f'Northing coordinates in UTM 33N')\n",
    "ax.set_xlabel(f'Easting coordinates in UTM 33N');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get indices defining the train bounding boxes, as these will be used to name our EOPatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eops_train = bboxes.eop_index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eops_train), np.sort(eops_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Sentinel-2 time-series <a name=\"sentinel-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input images are Sentinel-2 L2A acquired over a 6-months period, from March 1st to September 1st 2019. \n",
    "\n",
    "The following data will be downloaded:\n",
    "\n",
    " * `BANDS`: all 12 [Sentinel-2 multi-spectral L2A bands](https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/product-types/level-2a). The bands are in the following order `[B01, B02, B03, B04, B05, B06, B07, B08, B8A, B09, B11, B12]`. The bands that have original spatial resolution of `20m` and `60m` are interpolated with a nearest-neighbour method to a `10m` resolution. More information about the interpolation process [here](https://docs.sentinel-hub.com/api/latest/data/sentinel-2-l2a/#processing-options). The bands will be downloaded and stored on disk as Digital Numbers (DNs), ranging from 0 to 10000;\n",
    " * `SCL`: Sen2Cor scene classification mask;\n",
    " * `IS_DATA`: binary mask denoting data/no-data pixels;\n",
    " * `CLP`, `CLM`: `s2cloudless` cloud probability and cloud mask. More details about these layers can be found [here](https://docs.sentinel-hub.com/api/latest/user-guides/cloud-masks/).\n",
    " \n",
    "More details about the bands can be found [here](https://docs.sentinel-hub.com/api/latest/data/sentinel-2-l2a/#available-bands-and-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download parameters - DO NOT CHANGE\n",
    "S2_TIME_INTERVAL = ('2019-03-01','2019-09-01')\n",
    "\n",
    "S2_RESOLUTION = 10  # metres\n",
    "S2_MAXCC = 0.5\n",
    "S2_TIME_DELTA = 120\n",
    "\n",
    "MAX_THREADS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example download for the first bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = BBox(bboxes.iloc[0].geometry, crs=CRS(bboxes.crs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eo-learn` task to download images from SentinelHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_s2_l2a = SentinelHubInputTask(\n",
    "    bands_feature=(FeatureType.DATA, 'BANDS'),\n",
    "    bands_dtype=np.uint16,\n",
    "    resolution=S2_RESOLUTION,\n",
    "    maxcc=S2_MAXCC,\n",
    "    time_difference=dt.timedelta(minutes=S2_TIME_DELTA),\n",
    "    data_collection=DataCollection.SENTINEL2_L2A,\n",
    "    additional_data=[(FeatureType.MASK, 'dataMask', 'IS_DATA'),\n",
    "                     (FeatureType.MASK, 'SCL'),\n",
    "                     (FeatureType.MASK, 'CLM'),\n",
    "                     (FeatureType.DATA, 'CLP')],\n",
    "    max_threads=MAX_THREADS,\n",
    "    config=sh_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download images for given `bbox` and `time_interval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s2_l2a_eop = get_s2_l2a.execute(bbox=bbox, time_interval=S2_TIME_INTERVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now stored in an [EOPatch](https://eo-learn.readthedocs.io/en/latest/examples/core/CoreOverview.html#EOPatch), where each raster feature is stored as a `numpy` array, vector data stored as a `geopandas` dataframe, while dates as `datetime` objects.\n",
    "\n",
    "This `EOPatch` contains 38 time frames, of `500x500` spatial dimension. All `EOPatches` will have the same spatial dimensions, but different temporal frames.\n",
    "\n",
    "**NOTE:** Band values are stored as digital numbers `DN`s, therefore they need to be multiplied by the `NORM_FACTORS` (i.e. `1e-4`) to derive reflectances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_l2a_eop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl_dict =  {\n",
    "    0: [0, 0, 0],  # no data\n",
    "    1: [1, 0, 0.016],  # saturated / defected\n",
    "    2: [0.525, 0.525, 0.525],  # dark area pixels\n",
    "    3: [0.467, 0.298, 0.043],  # cloud shadows\n",
    "    4: [0.063, 0.827, 0.176],  # vegetation\n",
    "    5: [1, 1, 0.325],  # bare soils\n",
    "    6: [0, 0, 1],  # water\n",
    "    7: [0.506, 0.506, 0.506],  # clouds low probability / unclassified \n",
    "    8: [0.753, 0.753, 0.753],  # clouds medium probability\n",
    "    9: [0.949, 0.949, 0.949],  # clouds high probability\n",
    "    10: [0.733, 0.773, 0.925],  # cirrus\n",
    "    11: [0.325, 1, 0.980]  # ice / snow\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise data in `EOPatch`. The axis represent UTM 33N coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose one time index from [0..37] to visualise\n",
    "tidx = 28\n",
    "\n",
    "vis_factor = 3.5\n",
    "\n",
    "norm_factor = s2_l2a_eop.scalar['NORM_FACTORS'][tidx]\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(25, 6), ncols=4, sharey=True)\n",
    "\n",
    "axs[0].imshow(vis_factor * norm_factor * s2_l2a_eop.data['BANDS'][tidx][..., [3, 2, 1]],\n",
    "              extent=get_extent(s2_l2a_eop))\n",
    "axs[0].set_title(f'S2 L2A - {s2_l2a_eop.timestamp[tidx]}')\n",
    "axs[0].set_ylabel(f'Northing coordinates in UTM 33N')\n",
    "axs[0].set_xlabel(f'Easting coordinates in UTM 33N')\n",
    "\n",
    "axs[1].imshow(s2_l2a_eop.mask['IS_DATA'][tidx].squeeze(), \n",
    "              vmin=0, vmax=1, extent=get_extent(s2_l2a_eop))\n",
    "axs[1].set_title('IS_DATA')\n",
    "axs[1].set_xlabel(f'Easting coordinates in UTM 33N')\n",
    "\n",
    "axs[2].imshow(s2_l2a_eop.mask['SCL'][tidx].squeeze(), \n",
    "              extent=get_extent(s2_l2a_eop), \n",
    "              cmap=ListedColormap([np.array(v) for v in scl_dict.values()]),\n",
    "              vmin=0, vmax=11)\n",
    "axs[2].set_title('SCL')\n",
    "axs[2].set_xlabel(f'Easting coordinates in UTM 33N')\n",
    "\n",
    "axs[3].imshow(s2_l2a_eop.data['CLP'][tidx].squeeze()/255, vmin=0, vmax=1,\n",
    "              extent=get_extent(s2_l2a_eop))\n",
    "axs[3].set_title('s2cloudless CLP')\n",
    "axs[3].set_xlabel(f'Easting coordinates in UTM 33N')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information in the provided masks can be used to mask out invalid frames containing clouds/snow/fog. Below is an example using `IS_DATA` and `CLM` mask to find the percentage of valid data in the 6-months period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = np.mean(s2_l2a_eop.mask['IS_DATA'] & ~s2_l2a_eop.mask['CLM'], \n",
    "                     axis=(1,2,3)) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12.5, 7))\n",
    "ax.plot(s2_l2a_eop.timestamp, valid_data)\n",
    "ax.set_ylabel('Fraction of valid data')\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of frames for this EOPatch with a fraction of valid data greater than 90%. The `SCL` mask and `CLP` masks can be similarly used to estimate data validity fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(valid_data > .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Reference polygons <a name=\"reference\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the Sentinel-2 time-series data, time to add the information about the target _cultivated land_ map. This mask is derived from a combination of a _land cover_ dataset and a _declared crops_ dataset.\n",
    "\n",
    "The reference data is stored as polygons in a GeoDB table. The following task retrieves the polygons within each bounding box defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_polys = GeoDBVectorImportTask(\n",
    "    feature=(FeatureType.VECTOR_TIMELESS, 'REFERENCE'), \n",
    "    geodb_client=client, \n",
    "    geodb_collection='ai4eo_reference',\n",
    "    geodb_db=GEODB_DATABASE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the task and create a new `VECTOR_TIMELESS` feature holding the polygons as a `geopandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eop = get_polys.execute(eopatch=s2_l2a_eop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eop.vector_timeless['REFERENCE'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each polygon has a `label` attribute which denotes its land cover. Values between 1 and 30 denote _agricultural land_ including grassland and meadows, values above 1000 denote land cover types like _water_, _forest_ and _build-up_ area, while the label 100 denotes _agricultural land_ of which the crop-type is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eop.vector_timeless['REFERENCE'].label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eop.vector_timeless['REFERENCE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "df.query('label<=30').plot(ax=ax, color='xkcd:dull yellow', alpha=.8)\n",
    "df.query('label<=30').boundary.plot(ax=ax, color='xkcd:dull yellow',\n",
    "                                    alpha=.8, linewidth=.5, label='agricultural')\n",
    "\n",
    "df.query('label==100').plot(ax=ax, color='xkcd:black', alpha=.8)\n",
    "df.query('label==100').boundary.plot(ax=ax, color='xkcd:black',\n",
    "                                     alpha=.8, linewidth=.5, label='not declared')\n",
    "\n",
    "df.query('(label>1000) and (label<2000)').plot(ax=ax, color='xkcd:orange', alpha=.8);\n",
    "df.query('(label>1000) and (label<2000)').boundary.plot(ax=ax, color='xkcd:orange',\n",
    "                                                        alpha=.8, linewidth=.5, label='shrubland')\n",
    "\n",
    "df.query('label==2000').plot(ax=ax, color='xkcd:jungle green', alpha=.8)\n",
    "df.query('label==2000').boundary.plot(ax=ax, color='xkcd:jungle green',\n",
    "                                      alpha=.8, linewidth=.5, label='forest')\n",
    "\n",
    "df.query('label==3000').plot(ax=ax, color='xkcd:red', alpha=.8);\n",
    "df.query('label==3000').boundary.plot(ax=ax, color='xkcd:red', alpha=.8,\n",
    "                                      linewidth=.5, label='built-up')\n",
    "\n",
    "# # no values in this EOPatch\n",
    "# df.query('(label>4000) and (label<=5000)').plot(ax=ax, color='xkcd:bright teal', alpha=.8)\n",
    "# df.query('(label>4000) and (label<=5000)').plot(ax=ax, color='xkcd:bright teal', alpha=.8,\n",
    "#                                                 linewidth=.5, label='wetland')\n",
    "\n",
    "# # no values in this EOPatch\n",
    "# df.query('label==6000').plot(ax=ax, color='xkcd:grey', alpha=.8)\n",
    "# df.query('label==6000').plot(ax=ax, color='xkcd:grey', alpha=.8, linewidth=.5, label='bareland')\n",
    "\n",
    "df.query('label==7000').plot(ax=ax, color='xkcd:blue', alpha=.8)\n",
    "df.query('label==7000').plot(ax=ax, color='xkcd:blue', alpha=.8, linewidth=.5, label='water')\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.set_ylabel(f'Northing coordinates in UTM 33N')\n",
    "ax.set_xlabel(f'Easting coordinates in UTM 33N');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names of labels and labels mapping to a binary _cultivated/not-cultivated_ mask are provided in the `json` files in the `metadata` folder. \n",
    "\n",
    "In the context of this challenge, _cultivated land_ denotes arable land that is typically worked by ploughing, sowing and raising crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metadata/cultivated-mapping.json') as jfile:\n",
    "    cultivated_mapping = {int(k): v for k, v in json.load(jfile).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels that will be mapped to _non-cultivated land_. These labels are a union of _agricultural land_ that is **not** cultivated, e.g. grassland/meadows, tree plantations, greenhouses, and _land covers_ that are not _agricultural land_, e.g. build-up area, water, forest, shrubland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cultivated_mapping[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels that will be mapped to _cultivated land_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cultivated_mapping[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special class which denotes agricultural land that is **not** declared as a specific _cultivated land_. These pixels will have similarities to _cultivated land_, although information is missing about the exact parcel features. For this reason, these polygons will have a weight of `0` and therefore will **not** be used to compute the validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cultivated_mapping[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display names of crops and land cover for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metadata/label-names.json') as jfile:\n",
    "    label_names = {int(k): v for k, v in json.load(jfile).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Not-cultivated land_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ncl in cultivated_mapping[0]:\n",
    "    prefix = 'Crop-types' if ncl < 100 else 'Land cover'\n",
    "    print(f'{prefix} for label {ncl}:')\n",
    "    print(label_names[ncl])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Cultivated land_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in cultivated_mapping[1]:\n",
    "    prefix = 'Crop-types' if cl < 100 else 'Land cover'\n",
    "    print(f'{prefix} for label {cl}:')\n",
    "    print(label_names[cl])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** although these labels are mapped to a binary mask for training, you can use label information for sampling of pixels and image chips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** there is a very small number of polygons where crop declarations disagree with the land cover information, i.e. there is a declared cultivated land where the land cover indicates it is not agricultural. We leave the choice to you on how to deal with these polygons, e.g. whether to consider them as _cultivated_ or _not-cultivated_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can rasterize the reference labels, to allow further processing and model training. **The reference polygons are rasterised at the target spatial resolution of 2.5m**, resulting in a `4x` spatial enhancement. \n",
    "\n",
    "Three masks are added to the `EOPatch`:\n",
    "\n",
    " * `CULTIVATED`: this mask is the target output used in training and testing of your method;\n",
    " * `NOT_DECLARED`: this mask marks pixels that are ignored in the calculation of the validation metrics;\n",
    " * `ALL_POLYS`: all polygons are rasterised with a given `label`. Use this to customise your sampling given land cover and crop-type information.\n",
    " \n",
    "A negative buffer of 2.5m is applied prior to rasterisation, in order to single out neighbouring touching polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_RESOLUTION = 2.5 # metres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_polys_to_mask = VectorToRaster(\n",
    "    (FeatureType.VECTOR_TIMELESS, 'REFERENCE'),\n",
    "    (FeatureType.MASK_TIMELESS, 'CULTIVATED'),\n",
    "    values=cultivated_mapping[1],\n",
    "    values_column='label',\n",
    "    raster_resolution=MAP_RESOLUTION,\n",
    "    raster_dtype=np.uint8,  \n",
    "    all_touched=False,\n",
    "    buffer=-MAP_RESOLUTION  # DO NOT CHANGE THIS BUFFER\n",
    ")\n",
    "\n",
    "invalid_polys_to_mask = VectorToRaster(\n",
    "    (FeatureType.VECTOR_TIMELESS, 'REFERENCE'),\n",
    "    (FeatureType.MASK_TIMELESS, 'NOT_DECLARED'),\n",
    "    values=cultivated_mapping[2],\n",
    "    values_column='label',\n",
    "    raster_resolution=MAP_RESOLUTION,\n",
    "    raster_dtype=np.uint8,\n",
    "    all_touched=False,\n",
    "    buffer=-MAP_RESOLUTION  # DO NOT CHANGE THIS BUFFER\n",
    ")\n",
    "\n",
    "all_polys_to_mask = VectorToRaster(\n",
    "    (FeatureType.VECTOR_TIMELESS, 'REFERENCE'),\n",
    "    (FeatureType.MASK_TIMELESS, 'ALL_POLYS'),\n",
    "    values=None,\n",
    "    values_column='label',\n",
    "    raster_resolution=MAP_RESOLUTION,\n",
    "    raster_dtype=np.uint16,\n",
    "    all_touched=False,\n",
    "    buffer=-MAP_RESOLUTION  # you can modify this buffer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eop = valid_polys_to_mask.execute(eop)\n",
    "eop = invalid_polys_to_mask.execute(eop)\n",
    "eop = all_polys_to_mask.execute(eop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.imshow(eop.mask_timeless['CULTIVATED'].squeeze(), \n",
    "          vmin=0, vmax=30, extent=get_extent(eop), \n",
    "          interpolation='nearest', cmap=plt.cm.tab20c)\n",
    "ax.set_ylabel(f'Northing coordinates in UTM 33N')\n",
    "ax.set_xlabel(f'Easting coordinates in UTM 33N');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above tasks rasterise polygons assigning to pixels their `label` values. We add a task that turns the `CULTIVATED` and `NOT_DECLARED` masks into binary masks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToBinaryTask(EOTask):\n",
    "    \"\"\" Turn a discrete feature to a binary mask \"\"\"\n",
    "    def __init__(self, feature: Tuple[FeatureType, str]):\n",
    "        \"\"\" \n",
    "        :param feature: Feature in eopatch to binarise \n",
    "        \"\"\"\n",
    "        self.feature = feature\n",
    "        \n",
    "    def execute(self, eopatch: EOPatch) -> EOPatch:\n",
    "        \"\"\" Overwrite existing feature with binary mask \"\"\"\n",
    "        eopatch[self.feature] = eopatch[self.feature] > 0\n",
    "        return eopatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_to_binary = ToBinaryTask((FeatureType.MASK_TIMELESS, 'CULTIVATED'))\n",
    "invalid_to_binary = ToBinaryTask((FeatureType.MASK_TIMELESS, 'NOT_DECLARED'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eop = valid_to_binary.execute(eop)\n",
    "eop = invalid_to_binary.execute(eop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display target binary _cultivated land_ map, overlayed with the original polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "ax.imshow(eop.mask_timeless['CULTIVATED'], \n",
    "          interpolation='nearest', \n",
    "          cmap=plt.cm.binary_r, extent=get_extent(eop))\n",
    "df[df.label.isin(cultivated_mapping[1])].plot(ax=ax, \n",
    "                                              color='xkcd:ivory', \n",
    "                                              alpha=.4, \n",
    "                                              linewidth=.5)\n",
    "\n",
    "ax.set_ylabel(f'Northing coordinates in UTM 33N')\n",
    "ax.set_xlabel(f'Easting coordinates in UTM 33N');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Training data download <a name=\"train-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a workflow that downloads and processes the training data as shown above, and parallelize the workflow over the 100 training bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this path accordingly to your set-up\n",
    "EOPATCHES_PATH = 'eopatches'\n",
    "EOPATCHES_TRAIN_PATH = f'{EOPATCHES_PATH}/train/'\n",
    "EOPATCHES_TEST_PATH = f'{EOPATCHES_PATH}/test/'\n",
    "\n",
    "if not os.path.exists(EOPATCHES_TRAIN_PATH):\n",
    "    os.makedirs(EOPATCHES_TRAIN_PATH, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(EOPATCHES_TEST_PATH):\n",
    "    os.makedirs(EOPATCHES_TEST_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task to save the `EOPatch`es to disk for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = SaveTask(path=EOPATCHES_PATH, compress_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous steps are combined into a linear workflow, performing S2 data download, adding the reference polygons, rasterising the polygons to rasters, creating binary masks and saving the eopatches to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = LinearWorkflow(get_s2_l2a, \n",
    "                          get_polys, \n",
    "                          valid_polys_to_mask,\n",
    "                          invalid_polys_to_mask,\n",
    "                          all_polys_to_mask,\n",
    "                          valid_to_binary,\n",
    "                          invalid_to_binary, \n",
    "                          save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_args = []\n",
    "\n",
    "for irow, row in tqdm(bboxes.iterrows(), total=len(bboxes)):\n",
    "    bbox = BBox(row.geometry, crs=CRS(bboxes.crs))\n",
    "    eopatch_folder = f'train/eopatch-{row.eop_index}'\n",
    "    \n",
    "    eop_exec_args = {\n",
    "        get_s2_l2a: {\n",
    "            'bbox': bbox,\n",
    "            'time_interval': S2_TIME_INTERVAL\n",
    "        },\n",
    "        save: {\n",
    "            'eopatch_folder': eopatch_folder\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    execution_args.append(eop_exec_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_args[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelise the workflow over the workers. Change `NUM_WORKERS` according to your set-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 4\n",
    "\n",
    "executor = EOExecutor(workflow, execution_args, save_logs=True, logs_folder='.')\n",
    "\n",
    "executor.run(workers=NUM_WORKERS)\n",
    "\n",
    "executor.make_report()\n",
    "\n",
    "failed_ids = executor.get_failed_executions()\n",
    "if failed_ids:\n",
    "    raise RuntimeError(f'Execution failed EOPatches with IDs:\\n{failed_ids}\\n'\n",
    "                       f'For more info check report at {executor.get_report_filename()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** now that all data has been saved to disk, there is no need to run the code above for download. To read the `EOPatch`es, simply use the `LoadTask` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = LoadTask(path=EOPATCHES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eop_rand = bboxes.sample(n=1).iloc[0]\n",
    "\n",
    "eop_folder = f'train/eopatch-{eop_rand.eop_index}'\n",
    "\n",
    "reop = load.execute(eopatch_folder=eop_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Testing data download <a name=\"test-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above showed how to retrieve data to be used to train your algorithms. The data used to test and evaluate your algorithm is instead provided as a single compressed file to download. Check the main webpage to find the link to download the file (appr. 2.7GB).\n",
    "\n",
    "This is done to anonymize the location of the test areas. Such 25 test areas have been taken from Slovenia and from neighbouring countries. The distribution of bands, labels and underlying cultivated land for the test areas matches the distributions of the train areas. \n",
    "\n",
    "The test EOPatches are named as `eopatch-01` to `eopatch-25`, and have been assigned random spatial coordinates all over UTM 33N. The EOPatches contain only information about the bands, clouds and scene classification masks. The aim of the challenge is to predict and submit a binary _cultivated land_ map at 2.5m for these 25 test areas. \n",
    "\n",
    "The following extracts the downloaded `test-eopatches.tar.gz` file in the `test` subfolder of our `EOPATCHES_PATH` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xvzf test-eopatches.tar.gz {EOPATCHES_PATH}/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {EOPATCHES_PATH}/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eops_test = sorted(os.listdir(f'{EOPATCHES_PATH}/test/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the content of a test `EOPatch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teop = load.execute(eopatch_folder=f'test/eopatch-01')\n",
    "\n",
    "teop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Files check <a name=\"files-check\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you downloaded the training data through the APIs or direct link, best to check that all data has been correctly retrieved.\n",
    "\n",
    "The same applies for the test data downloaded through direct link.\n",
    "\n",
    "The following code encodes each downloaded numpy file as MD5 hash and compares them to the target MD5 hashes provided in the `metadata` folder. It should take a few minutes to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOPATCHES_PATH = '/work/ka1176/shared_data/2021-ai4eo/eopatches/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_md5_hashes = md5_encode_files(f'{EOPATCHES_PATH}/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_md5_hashes = md5_encode_files(f'{EOPATCHES_PATH}/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following fails, there are some issues with the downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_md5_hashes.equals(pd.read_csv('metadata/hashed-md5-train-files.csv', \n",
    "                                           usecols=['filename', 'hash']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_md5_hashes.equals(pd.read_csv('metadata/hashed-md5-test-files.csv', \n",
    "                                          usecols=['filename', 'hash']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validation Metric <a name=\"validation-metric\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section reports how the validation metric is computed, to allow you to implement the same logic when optimising your algorithm.\n",
    "\n",
    "The metric is a weighted version of the [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient). \n",
    "\n",
    "The following functions shows how the weight map is obtained from the `CULTIVATED` and `NOT_DECLARED` masks. The idea behind this weighting scheme is to provide more weight, e.g. relevance, to small parcels, and to the contouring background that singles them out. Ideally, your algorithm should be able to classify and recover all parcels, in particular the smaller ones since they are the most challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import skewnorm\n",
    "\n",
    "from skimage import measure\n",
    "from skimage.morphology import binary_dilation, disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting_function(pix_size: int, median_pix_size: int, highest_weight_pix_size: int = 35,\n",
    "                       skewness: int = 15) -> float:\n",
    "    \"\"\" Creates weight to be applied to a parcel depending on its number of pixels (after pixelation) \"\"\"\n",
    "    if pix_size >= median_pix_size:\n",
    "        return 1\n",
    "    \n",
    "    xs = np.linspace(1, median_pix_size, median_pix_size)\n",
    "    y1 = skewnorm.pdf(xs, skewness, loc=highest_weight_pix_size-100/3.14, scale=100)\n",
    "    y1 = y1 / max(y1)\n",
    "    y1 = y1 + 1\n",
    "\n",
    "    return y1[int(pix_size)].astype(np.float)\n",
    "\n",
    "\n",
    "class AddWeightMapTask(EOTask):\n",
    "    \"\"\" Computes the weight map used to compute the validation metric \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 cultivated_feature: Tuple[FeatureType, str], \n",
    "                 not_declared_feature: Tuple[FeatureType, str], \n",
    "                 weight_feature: Tuple[FeatureType, str], \n",
    "                 radius: int = 2, seed: int = 4321):\n",
    "        self.cultivated_feature = cultivated_feature\n",
    "        self.not_declared_feature = not_declared_feature\n",
    "        self.weight_feature = weight_feature\n",
    "        self.radius = radius\n",
    "        self.seed = seed\n",
    "        \n",
    "    def execute(self, eopatch: EOPatch) -> EOPatch:\n",
    "        cultivated = eopatch[self.cultivated_feature].astype(np.uint8).squeeze()\n",
    "        not_declared = eopatch[self.not_declared_feature].squeeze()\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        # compute connected components on binary mask\n",
    "        conn_comp = measure.label(cultivated, background=0)\n",
    "        # number of connected components\n",
    "        n_comp = np.max(conn_comp) + 1\n",
    "\n",
    "        # Placeholder for outputs\n",
    "        height, width = cultivated.shape\n",
    "        weights = np.zeros((height, width), dtype=np.float32)\n",
    "        contours = np.zeros((height, width), dtype=np.float32)\n",
    "        counts = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "        # Loop over connected components, ignoring background\n",
    "        for ncc in tqdm(np.arange(1, n_comp)):\n",
    "            parcel_mask = conn_comp == ncc\n",
    "            # number of pixels of each component, i.e. parcel\n",
    "            n_pixels = np.sum(parcel_mask)\n",
    "\n",
    "            # compute external boundary of parcel \n",
    "            dilated_mask = binary_dilation(parcel_mask, selem=disk(radius=self.radius))\n",
    "            contour = np.logical_and(~parcel_mask, dilated_mask)\n",
    "\n",
    "            weight = weighting_function(n_pixels, median_pix_size=400)\n",
    "\n",
    "            weights[parcel_mask] = weight\n",
    "            contours += 2 * weight * contour\n",
    "            # In case countours overlap, the average weight is taken\n",
    "            counts += contour\n",
    "\n",
    "        # combine weights from all parcels into a single map. First add (averaged) contours,\n",
    "        # then weighted parcels, then background \n",
    "        weight_map = np.zeros((height, width), dtype=np.float32)\n",
    "        weight_map[contours > 0] = contours[contours > 0] / counts[contours > 0]\n",
    "        weight_map[weights > 0] = weights[weights > 0]\n",
    "        weight_map[weight_map == 0] = 1\n",
    "\n",
    "        # add zero weights at border and undeclared parcels\n",
    "        weight_map[not_declared == True] = 0\n",
    "        weight_map[:1, :] = 0\n",
    "        weight_map[:, :1] = 0\n",
    "        weight_map[-2:, :] = 0\n",
    "        weight_map[:, -2:] = 0\n",
    "\n",
    "        eopatch[self.weight_feature] = weight_map[..., np.newaxis]\n",
    "        \n",
    "        return eopatch\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighting functions is defined as follows:\n",
    "\n",
    " * `w=1`, if label is `1` and number of pixels is larger than the median, where the median is 400\n",
    " * `w=1`, if label is `0`, e.g. background\n",
    " * `w=0`, if `NOT_DECLARED` mask is `True`\n",
    " * otherwise, a skewed normal distribution as a function of the number of pixels is returned, as shown below.\n",
    " \n",
    "The weight map is computed as follows:\n",
    "\n",
    "`SCORE_WEIGHT = (cultivated*weight) + (2*contours*weight)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(1, 400, 100)\n",
    "ys = [weighting_function(x, median_pix_size=400) for x in xs]\n",
    "\n",
    "fig , ax = plt.subplots(figsize=(15, 7))\n",
    "ax.plot(xs, ys)\n",
    "ax.set_xlabel('Size in pixels')\n",
    "ax.set_ylabel('Weight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_weight = AddWeightMapTask(\n",
    "    (FeatureType.MASK_TIMELESS, 'CULTIVATED'), \n",
    "    (FeatureType.MASK_TIMELESS, 'NOT_DECLARED'),\n",
    "    (FeatureType.DATA_TIMELESS, 'WEIGHTS')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reop = add_weight.execute(teop)\n",
    "reop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(30, 10), ncols=3, sharey=True)\n",
    "\n",
    "axs[0].imshow(reop.mask_timeless['CULTIVATED'].squeeze(), vmin=0, vmax=1, extent=get_extent(reop))\n",
    "axs[0].set_title('CULTIVATED')\n",
    "\n",
    "axs[1].imshow(reop.mask_timeless['NOT_DECLARED'].squeeze(), vmin=0, vmax=1, extent=get_extent(reop))\n",
    "axs[1].set_title('NOT_DECLARED')\n",
    "\n",
    "axs[2].imshow(reop.data_timeless['WEIGHTS'].squeeze(), vmin=0, vmax=4, extent=get_extent(reop))\n",
    "axs[2].set_title('WEIGHTS')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can loop over your training EOPatches and add the weight map.\n",
    "\n",
    "Once you aggregate the values for reference (e.g. `y_true`), predicted (e.g. `y_pred`) and weights (e.g. `sample_weight`) arrays, you can compute the metric as follows\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "score = matthews_corrcoef(y_true, y_pred, sample_weight=sample_weight)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Processing <a name=\"data-processing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section offers some tips and pointers on how to possibly transform the data in a ML-ready format.\n",
    "\n",
    "Possible steps are:\n",
    "\n",
    " * filter out invalid timestamps due to clouds/shadows/snow;\n",
    " * sample or generate the same number of S2 timestamps for all EOPatches;\n",
    " * sample smaller image chips from S2 time-series with corresponding target binary _cultivated land_ map.\n",
    " \n",
    "If you want to use `eo-learn` for these tasks, check out the documentation about existing tasks like filtering and pixel sampling [here](https://eo-learn.readthedocs.io/en/latest/eotasks.html). You can easily implement your own task (as done above) by following [this example](https://eo-learn.readthedocs.io/en/latest/examples/core/CoreOverview.html#EOTask). [Here](https://github.com/sentinel-hub/eo-learn/tree/master/examples) you can find a collection of examples including land cover and crop-type classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide a custom task to sample patchlets/image chips from the Sentinel-2 time-series and the reference map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplePatchletsTask(EOTask):\n",
    "\n",
    "    SCALE_FACTOR = 4\n",
    "\n",
    "    def __init__(self, s2_patchlet_size: int, num_samples: int):\n",
    "        \"\"\" Set-up of task \n",
    "        \n",
    "        :param s2_patchlet_size: Size in pixels of resulting patchlet\n",
    "        :param num_samples: Number of patchlets to sample\n",
    "        \"\"\"\n",
    "        self.s2_patchlet_size = s2_patchlet_size\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def _calculate_sampled_bbox(self, bbox: BBox, r: int, c: int, s: int,\n",
    "                                resolution: float) -> BBox:\n",
    "        \"\"\" Calculate bounding box of smaller patchlets \"\"\"\n",
    "        return BBox(((bbox.min_x + resolution * c,  bbox.max_y - resolution * (r + s)),\n",
    "                     (bbox.min_x + resolution * (c + s), bbox.max_y - resolution * r)),\n",
    "                    bbox.crs)\n",
    "\n",
    "    def _sample_s2(self, eop: EOPatch, row: int, col: int, size: int, \n",
    "                   resolution: float = 10):\n",
    "        \"\"\" Randomly sample a patchlet from the EOPatch \"\"\"\n",
    "        # create a new eopatch for each sub-sample\n",
    "        sampled_eop = EOPatch(timestamp=eop.timestamp, \n",
    "                              scalar=eop.scalar, \n",
    "                              meta_info=eop.meta_info)\n",
    "        \n",
    "        # sample S2-related arrays\n",
    "        features = eop.get_feature_list()\n",
    "        s2_features = [feature for feature in features \n",
    "                       if isinstance(feature, tuple) and \n",
    "                       (feature[0].is_spatial() and feature[0].is_time_dependent())]\n",
    "        \n",
    "        for feature in s2_features:\n",
    "            sampled_eop[feature] = eop[feature][:, row:row + size, col:col + size, :]\n",
    "        \n",
    "        # calculate BBox for new sub-sample\n",
    "        sampled_eop.bbox = self._calculate_sampled_bbox(eop.bbox, \n",
    "                                                        r=row, c=col, s=size, \n",
    "                                                        resolution=resolution)\n",
    "        sampled_eop.meta_info['size_x'] = size\n",
    "        sampled_eop.meta_info['size_y'] = size\n",
    "        \n",
    "        # sample from target maps, beware of `4x` scale factor\n",
    "        target_features = eop.get_feature(FeatureType.MASK_TIMELESS).keys()\n",
    "        \n",
    "        for feat_name in target_features:\n",
    "            sampled_eop.mask_timeless[feat_name] = \\\n",
    "            eop.mask_timeless[feat_name][self.SCALE_FACTOR*row:self.SCALE_FACTOR*row + self.SCALE_FACTOR*size, \n",
    "                                         self.SCALE_FACTOR*col:self.SCALE_FACTOR*col + self.SCALE_FACTOR*size]\n",
    "        \n",
    "        return sampled_eop\n",
    "\n",
    "    def execute(self, eopatch_s2: EOPatch, buffer: int=0,  seed: int=42) -> List[EOPatch]:\n",
    "        \"\"\" Sample a number of patchlets from the larger EOPatch. \n",
    "        \n",
    "        :param eopatch_s2: EOPatch from which patchlets are sampled\n",
    "        :param buffer: Do not sample in a given buffer at the edges of the EOPatch\n",
    "        :param seed: Seed to initialise the pseudo-random number generator\n",
    "        \"\"\"\n",
    "        _, n_rows, n_cols, _ = eopatch_s2.data['BANDS'].shape\n",
    "        np.random.seed(seed)\n",
    "        eops_out = []\n",
    "        \n",
    "        # random sampling of upper-left corner. Change this for non-overlapping patchlets\n",
    "        for patchlet_num in range(0, self.num_samples):\n",
    "            row = np.random.randint(buffer, n_rows - self.s2_patchlet_size - buffer)\n",
    "            col = np.random.randint(buffer, n_cols - self.s2_patchlet_size - buffer)\n",
    "            sampled_s2 = self._sample_s2(eopatch_s2, row, col, self.s2_patchlet_size)\n",
    "            eops_out.append(sampled_s2)\n",
    "\n",
    "        return eops_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_task = SamplePatchletsTask(s2_patchlet_size=32, num_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample_task.execute(eop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidx = 28\n",
    "sample = samples[2]\n",
    "norm_fact = sample.scalar['NORM_FACTORS'][tidx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(3.5 * norm_factor * sample.data['BANDS'][tidx][..., [3,2,1]], \n",
    "          extent=get_extent(sample))\n",
    "ax.imshow(sample.mask_timeless['CULTIVATED'].squeeze(), \n",
    "          vmin=0, vmax=1, alpha=.2, \n",
    "          extent=get_extent(sample));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Submission Example <a name=\"submission-example\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A valid submission entails submitting a zipped folder containing a tiff file for each of the 25 test EOPatches at the target **2.5m** resolution, meaning a `2000x2000` pixels binary raster for each test areas.\n",
    "\n",
    "The `.tif` file should be in the `epsg:32633` coordinate reference system, as the data provided, and should be named as the corresponding `EOPatch`, i.e. `eopatch-01.tif`. Check-out the [`ExportToTiff`](https://eo-learn.readthedocs.io/en/latest/eolearn.io.local_io.html#eolearn.io.local_io.ExportToTiff) task to export a feature to  a tif file. The task uses [rasterio](https://rasterio.readthedocs.io/en/latest/topics/writing.html).\n",
    "\n",
    "The following code shows how to generate a valid submission using a dummy thresholding method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Prepare a submission <a name=\"prepare-a-submission\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows how to run a simple workflow to export the predicted masks for submission. \n",
    "\n",
    "The workflow applies the following:\n",
    "\n",
    " * reads `EOPatch` from disk and transforms DNs to reflectances\n",
    " * filters timestamps based on `CLM` and `IS_DATA` masks\n",
    " * computes NDVI and mean NDVI over the filtered timestamps\n",
    " * upscales the mean NDVI to the target spatial resolution of 2.5m\n",
    " * applies a thresholding on upscaled mean NDVI\n",
    " * exports predicted binary mask\n",
    " \n",
    "The workflow is applied on test EOPatches since they are the only files evaluated.\n",
    "\n",
    "In your method of course you will make use of the training data to develop a machine learning model, and apply the model to the test patches for the prediction only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_DIR = 'submission'\n",
    "\n",
    "if not os.path.exists(SUBMISSION_DIR):\n",
    "    os.makedirs(SUBMISSION_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from eolearn.features import NormalizedDifferenceIndexTask, SimpleFilterTask\n",
    "from eolearn.mask import AddValidDataMaskTask\n",
    "from eolearn.io import ExportToTiff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeReflectances(EOTask):\n",
    "    \"\"\" Apply normalisation factors to DNs \"\"\"\n",
    "    def __init__(self, feature):\n",
    "        self.feature = feature\n",
    "        \n",
    "    def execute(self, eopatch):\n",
    "        eopatch[self.feature] = eopatch.scalar['NORM_FACTORS'][..., None, None] \\\n",
    "            * eopatch[self.feature].astype(np.float32)\n",
    "        return eopatch\n",
    "\n",
    "class SentinelHubValidData:\n",
    "    \"\"\"\n",
    "    Combine 'CLM' mask with `IS_DATA` to define a `VALID_DATA_SH` mask\n",
    "    The SentinelHub's cloud mask is asumed to be found in eopatch.mask['CLM']\n",
    "    \"\"\"\n",
    "    def __call__(self, eopatch):\n",
    "        return eopatch.mask['IS_DATA'].astype(bool) & np.logical_not(eopatch.mask['CLM'].astype(bool))\n",
    "\n",
    "    \n",
    "class AddValidCountTask(EOTask):\n",
    "    \"\"\"\n",
    "    The task counts number of valid observations in time-series and stores the results in the timeless mask.\n",
    "    \"\"\"\n",
    "    def __init__(self, count_what, feature_name):\n",
    "        self.what = count_what\n",
    "        self.name = feature_name\n",
    "\n",
    "    def execute(self, eopatch):\n",
    "        eopatch[(FeatureType.MASK_TIMELESS, self.name)] = np.count_nonzero(eopatch.mask[self.what], axis=0)\n",
    "        return eopatch\n",
    "    \n",
    "    \n",
    "class ValidDataFractionPredicate:\n",
    "    \"\"\" Predicate that defines if a frame from EOPatch's time-series is valid or not. Frame is valid if the\n",
    "    valid data fraction is above the specified threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, array):\n",
    "        coverage = np.sum(array.astype(np.uint8)) / np.prod(array.shape)\n",
    "        return coverage > self.threshold\n",
    "\n",
    "    \n",
    "class MeanNDVI(EOTask):\n",
    "    \"\"\"\n",
    "    Compute the mean temporal NDVI\n",
    "    \"\"\"\n",
    "    def __init__(self, ndvi_feature, mean_ndvi_feature):\n",
    "        self.ndvi_feature = ndvi_feature\n",
    "        self.mean_ndvi_feature = mean_ndvi_feature\n",
    "        \n",
    "    def execute(self, eopatch):\n",
    "        eopatch[self.mean_ndvi_feature] = np.nanmean(eopatch[self.ndvi_feature], axis=0)\n",
    "        return eopatch\n",
    "    \n",
    "\n",
    "class Enhance(EOTask):\n",
    "    \"\"\"\n",
    "    Upscale S2 to target resolution\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_in, feature_out, scale_factor=4):\n",
    "        self.feature_in = feature_in\n",
    "        self.feature_out = feature_out\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "    def execute(self, eopatch):\n",
    "        array_in = eopatch[self.feature_in]\n",
    "        if array_in.ndim == 4:\n",
    "            array_out = np.concatenate([cv2.resize(arr.squeeze(), \n",
    "                                                   None,\n",
    "                                                   fx=self.scale_factor, \n",
    "                                                   fy=self.scale_factor, \n",
    "                                                   interpolation = cv2.INTER_CUBIC) \n",
    "                                        for arr in array_in])\n",
    "        elif array_in.ndim == 3:\n",
    "            array_out = cv2.resize(array_in.squeeze(), None, \n",
    "                                   fx=self.scale_factor, \n",
    "                                   fy=self.scale_factor, \n",
    "                                   interpolation = cv2.INTER_CUBIC) \n",
    "        eopatch[self.feature_out] = array_out[..., np.newaxis]\n",
    "        return eopatch\n",
    "    \n",
    "\n",
    "class Classify(EOTask):\n",
    "    \"\"\" Threshold mean NDVI mask \"\"\"\n",
    "    def __init__(self, feature_in, feature_out, low_cut=-1., high_cut=1.):\n",
    "        self.feature_in = feature_in\n",
    "        self.feature_out = feature_out\n",
    "        self.low_cut = low_cut\n",
    "        self.high_cut = high_cut\n",
    "        \n",
    "    def execute(self, eopatch):\n",
    "        array_in = eopatch[self.feature_in]\n",
    "        \n",
    "        new_eopatch = EOPatch(bbox=eopatch.bbox)\n",
    "        \n",
    "        new_eopatch[self.feature_out] = np.logical_and(array_in > self.low_cut, \n",
    "                                                       array_in <= self.high_cut).astype(np.uint8)\n",
    "        return new_eopatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_names = ['B01','B02','B03','B04','B05','B06','B07','B08','B8A','B09','B11','B12']\n",
    "\n",
    "# LOAD EOPATCH\n",
    "load = LoadTask(path=EOPATCHES_PATH)\n",
    "\n",
    "# COMPUTE REFLECTANCES FROM DNs\n",
    "normalise = ComputeReflectances((FeatureType.DATA, 'BANDS'))\n",
    "\n",
    "# CALCULATING NEW FEATURES\n",
    "# NDVI: (B08 - B04)/(B08 + B04)\n",
    "ndvi = NormalizedDifferenceIndexTask((FeatureType.DATA, 'BANDS'), \n",
    "                                     (FeatureType.DATA, 'NDVI'),\n",
    "                                     [band_names.index('B08'), \n",
    "                                      band_names.index('B04')])\n",
    "\n",
    "# VALIDITY MASK\n",
    "# Validate pixels using SentinelHub's cloud detection mask and region of acquisition\n",
    "add_sh_validmask = AddValidDataMaskTask(SentinelHubValidData(), 'IS_VALID')\n",
    "\n",
    "# COUNTING VALID PIXELS\n",
    "# Count the number of valid observations per pixel using valid data mask\n",
    "add_valid_count = AddValidCountTask('IS_VALID', 'VALID_COUNT')\n",
    "\n",
    "# FILTER OUT CLOUDY SCENES\n",
    "# Keep timestamps with > 95% valid coverage\n",
    "valid_data_predicate = ValidDataFractionPredicate(0.95)\n",
    "filter_task = SimpleFilterTask((FeatureType.MASK, 'IS_VALID'), valid_data_predicate)\n",
    "\n",
    "# COMPUTE MEAN TEMPORAL NDVI\n",
    "# Compute mean NDVI values over the time-series\n",
    "mean_ndvi = MeanNDVI((FeatureType.DATA, 'NDVI'), \n",
    "                     (FeatureType.DATA_TIMELESS, 'MEAN_NDVI'))\n",
    "\n",
    "# UPSCALE MEAN NDVI USING BICUBIC INTERPOLATION\n",
    "# Apply the 4x upscaling by using bicubic interpolation\n",
    "enhance = Enhance((FeatureType.DATA_TIMELESS, 'MEAN_NDVI'),\n",
    "                  (FeatureType.DATA_TIMELESS, 'BIG_MEAN_NDVI'))\n",
    "\n",
    "# THRESHOLD UPSCALED MEAN NDVI\n",
    "# Apply thesholds to upscaled mean NDVI\n",
    "classify = Classify((FeatureType.DATA_TIMELESS, 'BIG_MEAN_NDVI'),\n",
    "                    (FeatureType.MASK_TIMELESS, 'PREDICTION'),\n",
    "                    low_cut=.4, high_cut=.6)\n",
    "\n",
    "# EXPORT PREDICTION AS TIFF\n",
    "# Export the predicted binary mask as tiff for submission\n",
    "# NOTE: make sure both 0s and 1s are correctly exported\n",
    "export = ExportToTiff(feature=(FeatureType.MASK_TIMELESS, 'PREDICTION'),\n",
    "                      folder=SUBMISSION_DIR, crs='epsg:32633',\n",
    "                      image_dtype=np.uint8, \n",
    "                      no_data_value=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_workflow = LinearWorkflow(load, \n",
    "                                normalise,\n",
    "                                ndvi,\n",
    "                                add_sh_validmask,\n",
    "                                add_valid_count,\n",
    "                                filter_task,\n",
    "                                mean_ndvi,\n",
    "                                enhance,\n",
    "                                classify,\n",
    "                                export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_args = []\n",
    "\n",
    "for eop_test in eops_test:\n",
    "    eop_exec_args = {\n",
    "        load: {'eopatch_folder': f'test/{eop_test}'},\n",
    "        export: {'filename': f'{eop_test}.tif'}\n",
    "    }\n",
    "    \n",
    "    execution_args.append(eop_exec_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 4\n",
    "\n",
    "executor = EOExecutor(dummy_workflow, execution_args, save_logs=True, logs_folder='.')\n",
    "\n",
    "executor.run(workers=NUM_WORKERS)\n",
    "\n",
    "executor.make_report()\n",
    "\n",
    "failed_ids = executor.get_failed_executions()\n",
    "if failed_ids:\n",
    "    raise RuntimeError(f'Execution failed EOPatches with IDs:\\n{failed_ids}\\n'\n",
    "                       f'For more info check report at {executor.get_report_filename()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al {SUBMISSION_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** remove any other file that is not a `.tif` file from the submission folder.\n",
    "\n",
    "**NOTE:** you can now compress (to a `.zip` or to a `.tar.gz` file) the `submission` folder and submit it for evaluation.\n",
    "\n",
    "**NOTE:** make sure all pixels in the submitted tiffs are valid pixels, e.g. they are not set as `NO_DATA`.\n",
    "\n",
    "**NOTE:** the scoring system will perform the following checks on the submitted files:\n",
    "   * assert that the submission contains 25 files with the correct name, e.g. `eopatch-01.tif` to `eopatch-25.tif`;\n",
    "   * assert that each file has a single band of size `2000x2000` pixels;\n",
    "   * assert that the bounding box coordinates match the corresponding anonymised ones. Although the coordinates are randomised, working with geo-referenced images allows us to visually inspect them and compared them to the reference labels in any GIS software;\n",
    "   * assert that the predicted images are binary, i.e. the only values it contains are `[0, 1]`.\n",
    "   \n",
    "Make sure your submitted files respect the points above, otherwise the scoring system will throw an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C {SUBMISSION_DIR} -zcvf submission.tar.gz ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Submitting this dummy method should give a score of `0.53402575848615` on the public leaderboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new kernel",
   "language": "python",
   "name": "new-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
