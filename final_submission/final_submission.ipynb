{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective and Motivation\n",
    "\n",
    "Agriculture plays a central role globally by providing food for humans and livestock or material for industrial processes. It is a driver of economic growth and will play an essential role in reducing the impact of climate change and greening our economies. Accurate and reliable agricultural data is therefore paramount to ensure food security and global subsistence.\n",
    "\n",
    "<img src=\"ai4eo.png\" alt=\"example image of input an output\" title=\"Fig.1: Example of low resolution (10m) input image from Sentinental and high resolution (2.5m) binary output map\" width=\"1500\"/>\n",
    "\n",
    "The goal of this challenge is to map cultivated land using Copernicus Sentinel imagery, and to develop solutions to extract as much information as possible from the native 10-meter per pixel resolution. The area of interest for this challenge is Slovenia and the training images' area are shown in Fig. 1. An example input image for one time step from Sentinel imagery and the corresponding high-resolution output map is shown in Fig. 1. \n",
    "\n",
    "**Task:** Estimate a cultivated land binary map at 2.5 metres spatial resolution given as input a Sentinel-2 time-series at 10 metres spatial resolution, therefore resulting in a 4x spatial resolution enhancement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "The following cell contains all the modules needed and used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in modules\n",
    "import os\n",
    "import json\n",
    "import datetime as dt\n",
    "from typing import Tuple, List\n",
    "import argparse\n",
    "\n",
    "# Basics of Python data handling and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Math modules\n",
    "from scipy.stats import skewnorm\n",
    "import math\n",
    "\n",
    "# Module for GeoDB\n",
    "from xcube_geodb.core.geodb import GeoDBClient\n",
    "\n",
    "# Imports from eo-learn and sentinelhub-py\n",
    "from sentinelhub import CRS, BBox, SHConfig, DataCollection\n",
    "\n",
    "from eolearn.core import (FeatureType,\n",
    "                          EOPatch, \n",
    "                          EOTask, \n",
    "                          LinearWorkflow, \n",
    "                          EOExecutor, \n",
    "                          LoadTask,\n",
    "                          SaveTask)\n",
    "from eolearn.io import GeoDBVectorImportTask, SentinelHubInputTask, ExportToTiff\n",
    "from eolearn.geometry import VectorToRaster\n",
    "from eolearn.features import NormalizedDifferenceIndexTask, SimpleFilterTask\n",
    "from eolearn.mask import AddValidDataMaskTask\n",
    "\n",
    "# algorithms for image processing and computer vision\n",
    "from skimage import measure\n",
    "from skimage.morphology import binary_dilation, disk\n",
    "from PIL import Image\n",
    "\n",
    "# Modules for modelling\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOTasks\n",
    "\n",
    "The following cell contains all EOTasks used. Some are taken from the starter notebook and some have been defined by ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplePatchletsTask(EOTask):\n",
    "    '''\n",
    "    Sample patchlets from EOTask\n",
    "    '''\n",
    "\n",
    "    SCALE_FACTOR = 4 # do not change\n",
    "\n",
    "    def __init__(self, s2_patchlet_size: int, num_samples: int, random_mode: bool):\n",
    "        \"\"\" Set-up of task \n",
    "        \n",
    "        :param s2_patchlet_size: Size in pixels of resulting patchlet\n",
    "        :param num_samples: Number of patchlets to sample\n",
    "        \"\"\"\n",
    "        self.s2_patchlet_size = s2_patchlet_size\n",
    "        self.num_samples = num_samples\n",
    "        self.random_mode = random_mode\n",
    "\n",
    "    def _calculate_sampled_bbox(self, bbox: BBox, r: int, c: int, s: int,\n",
    "                                resolution: float) -> BBox:\n",
    "        \"\"\" Calculate bounding box of smaller patchlets \"\"\"\n",
    "        return BBox(((bbox.min_x + resolution * c,  bbox.max_y - resolution * (r + s)),\n",
    "                     (bbox.min_x + resolution * (c + s), bbox.max_y - resolution * r)),\n",
    "                    bbox.crs)\n",
    "\n",
    "    def _sample_s2(self, eop: EOPatch, row: int, col: int, size: int, \n",
    "                   resolution: float = 10):\n",
    "        \"\"\" Randomly sample a patchlet from the EOPatch \"\"\"\n",
    "        # create a new eopatch for each sub-sample\n",
    "        sampled_eop = EOPatch(timestamp=eop.timestamp, \n",
    "                              scalar=eop.scalar, \n",
    "                              meta_info=eop.meta_info)\n",
    "        \n",
    "        # sample S2-related arrays\n",
    "        features = eop.get_feature_list()\n",
    "        s2_features = [feature for feature in features \n",
    "                       if isinstance(feature, tuple) and \n",
    "                       (feature[0].is_spatial() and feature[0].is_time_dependent())]\n",
    "        \n",
    "        for feature in s2_features:\n",
    "            sampled_eop[feature] = eop[feature][:, row:row + size, col:col + size, :]\n",
    "        \n",
    "        # calculate BBox for new sub-sample\n",
    "        sampled_eop.bbox = self._calculate_sampled_bbox(eop.bbox, \n",
    "                                                        r=row, c=col, s=size, \n",
    "                                                        resolution=resolution)\n",
    "        sampled_eop.meta_info['size_x'] = size\n",
    "        sampled_eop.meta_info['size_y'] = size\n",
    "        \n",
    "        # sample from target maps, beware of `4x` scale factor\n",
    "        target_features = eop.get_feature(FeatureType.MASK_TIMELESS).keys()\n",
    "        \n",
    "        for feat_name in target_features:\n",
    "            sampled_eop.mask_timeless[feat_name] = \\\n",
    "            eop.mask_timeless[feat_name][self.SCALE_FACTOR*row:self.SCALE_FACTOR*row + self.SCALE_FACTOR*size, \n",
    "                                         self.SCALE_FACTOR*col:self.SCALE_FACTOR*col + self.SCALE_FACTOR*size]\n",
    "        \n",
    "        # sample from weight maps, beware of `4x` scale factor\n",
    "        target_features = eop.get_feature(FeatureType.DATA_TIMELESS).keys()\n",
    "        \n",
    "        for feat_name in target_features:\n",
    "            sampled_eop.data_timeless[feat_name] = \\\n",
    "            eop.data_timeless[feat_name][self.SCALE_FACTOR*row:self.SCALE_FACTOR*row + self.SCALE_FACTOR*size, \n",
    "                                         self.SCALE_FACTOR*col:self.SCALE_FACTOR*col + self.SCALE_FACTOR*size]\n",
    "        \n",
    "        return sampled_eop\n",
    "\n",
    "    def execute(self, eopatch_s2: EOPatch, buffer: int=0,  seed: int=42, random_mode: bool=1) -> List[EOPatch]:\n",
    "        \"\"\" Sample a number of patchlets from the larger EOPatch. \n",
    "        \n",
    "        :param eopatch_s2: EOPatch from which patchlets are sampled\n",
    "        :param buffer: Do not sample in a given buffer at the edges of the EOPatch\n",
    "        :param seed: Seed to initialise the pseudo-random number generator\n",
    "        :param random_mode: Select the upper left corner at random (default: True)\n",
    "        \"\"\"\n",
    "        _, n_rows, n_cols, _ = eopatch_s2.data['BANDS'].shape\n",
    "        np.random.seed(seed)\n",
    "        eops_out = []\n",
    "        \n",
    "        if not self.random_mode:\n",
    "            max_per_row = n_rows // self.s2_patchlet_size\n",
    "            max_per_col = n_cols // self.s2_patchlet_size\n",
    "        \n",
    "        # random sampling of upper-left corner. Added: Change this for non-overlapping patchlets\n",
    "        for patchlet_num in range(0, self.num_samples):\n",
    "            if self.random_mode:\n",
    "                row = np.random.randint(buffer, n_rows - self.s2_patchlet_size - buffer)\n",
    "                col = np.random.randint(buffer, n_cols - self.s2_patchlet_size - buffer)\n",
    "            else:\n",
    "                row = (buffer + patchlet_num // int(np.floor((n_rows - buffer) / self.s2_patchlet_size)) * self.s2_patchlet_size)\n",
    "                col = buffer + (patchlet_num * self.s2_patchlet_size) % (n_cols - buffer - self.s2_patchlet_size)\n",
    "                \n",
    "                row = (patchlet_num // max_per_row) * self.s2_patchlet_size\n",
    "                col = (patchlet_num % max_per_col) * self.s2_patchlet_size\n",
    "                \n",
    "                \n",
    "            sampled_s2 = self._sample_s2(eopatch_s2, row, col, self.s2_patchlet_size)\n",
    "            eops_out.append(sampled_s2)\n",
    "\n",
    "        return eops_out\n",
    "\n",
    "class ComputeReflectances(EOTask):\n",
    "    \"\"\" Apply normalisation factors to DNs (from starter notebook)\"\"\"\n",
    "    def __init__(self, feature):\n",
    "        self.feature = feature\n",
    "        \n",
    "    def execute(self, eopatch):\n",
    "        eopatch[self.feature] = eopatch.scalar['NORM_FACTORS'][..., None, None] \\\n",
    "            * eopatch[self.feature].astype(np.float32)\n",
    "        return eopatch\n",
    "\n",
    "class SentinelHubValidData:\n",
    "    \"\"\"\n",
    "    Combine 'CLM' mask with `IS_DATA` to define a `VALID_DATA_SH` mask\n",
    "    The SentinelHub's cloud mask is asumed to be found in eopatch.mask['CLM']\n",
    "    \"\"\"\n",
    "    def __call__(self, eopatch):\n",
    "        return eopatch.mask['IS_DATA'].astype(bool) & np.logical_not(eopatch.mask['CLM'].astype(bool))\n",
    "\n",
    "    \n",
    "class AddValidCountTask(EOTask):\n",
    "    \"\"\"\n",
    "    The task counts number of valid observations in time-series and stores the results in the timeless mask.\n",
    "    \"\"\"\n",
    "    def __init__(self, count_what, feature_name):\n",
    "        self.what = count_what\n",
    "        self.name = feature_name\n",
    "\n",
    "    def execute(self, eopatch):\n",
    "        eopatch[(FeatureType.MASK_TIMELESS, self.name)] = np.count_nonzero(eopatch.mask[self.what], axis=0)\n",
    "        return eopatch\n",
    "    \n",
    "    \n",
    "class ValidDataFractionPredicate:\n",
    "    \"\"\" Predicate that defines if a frame from EOPatch's time-series is valid or not. Frame is valid if the\n",
    "    valid data fraction is above the specified threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, array):\n",
    "        coverage = np.sum(array.astype(np.uint8)) / np.prod(array.shape)\n",
    "        return coverage > self.threshold\n",
    "\n",
    "class NanDataPredicate:\n",
    "    \"\"\" Predicate that defines if a frame from EOPatch's time-series contains nans --> invalid \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, array):\n",
    "        nancount = np.sum(np.isnan(array))\n",
    "        return nancount==0\n",
    "\n",
    "def weighting_function(pix_size: int, median_pix_size: int, highest_weight_pix_size: int = 35,\n",
    "                       skewness: int = 15) -> float:\n",
    "    \"\"\" Creates weight to be applied to a parcel depending on its number of pixels (after pixelation) \"\"\"\n",
    "    if pix_size >= median_pix_size:\n",
    "        return 1\n",
    "    \n",
    "    xs = np.linspace(1, median_pix_size, median_pix_size)\n",
    "    y1 = skewnorm.pdf(xs, skewness, loc=highest_weight_pix_size-100/3.14, scale=100)\n",
    "    y1 = y1 / max(y1)\n",
    "    y1 = y1 + 1\n",
    "\n",
    "    return y1[int(pix_size)].astype(np.float)\n",
    "\n",
    "\n",
    "class AddWeightMapTask(EOTask):\n",
    "    \"\"\" Computes the weight map used to compute the validation metric \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 cultivated_feature: Tuple[FeatureType, str], \n",
    "                 not_declared_feature: Tuple[FeatureType, str], \n",
    "                 weight_feature: Tuple[FeatureType, str], \n",
    "                 radius: int = 2, seed: int = 4321):\n",
    "        self.cultivated_feature = cultivated_feature\n",
    "        self.not_declared_feature = not_declared_feature\n",
    "        self.weight_feature = weight_feature\n",
    "        self.radius = radius\n",
    "        self.seed = seed\n",
    "        \n",
    "    def execute(self, eopatch: EOPatch) -> EOPatch:\n",
    "        cultivated = eopatch[self.cultivated_feature].astype(np.uint8).squeeze()\n",
    "        not_declared = eopatch[self.not_declared_feature].squeeze()\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        # compute connected components on binary mask\n",
    "        conn_comp = measure.label(cultivated, background=0)\n",
    "        # number of connected components\n",
    "        n_comp = np.max(conn_comp) + 1\n",
    "\n",
    "        # Placeholder for outputs\n",
    "        height, width = cultivated.shape\n",
    "        weights = np.zeros((height, width), dtype=np.float32)\n",
    "        contours = np.zeros((height, width), dtype=np.float32)\n",
    "        counts = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "        # Loop over connected components, ignoring background\n",
    "        for ncc in tqdm(np.arange(1, n_comp)):\n",
    "            parcel_mask = conn_comp == ncc\n",
    "            # number of pixels of each component, i.e. parcel\n",
    "            n_pixels = np.sum(parcel_mask)\n",
    "\n",
    "            # compute external boundary of parcel \n",
    "            dilated_mask = binary_dilation(parcel_mask, selem=disk(radius=self.radius))\n",
    "            contour = np.logical_and(~parcel_mask, dilated_mask)\n",
    "\n",
    "            weight = weighting_function(n_pixels, median_pix_size=400)\n",
    "\n",
    "            weights[parcel_mask] = weight\n",
    "            contours += 2 * weight * contour\n",
    "            # In case countours overlap, the average weight is taken\n",
    "            counts += contour\n",
    "\n",
    "        # combine weights from all parcels into a single map. First add (averaged) contours,\n",
    "        # then weighted parcels, then background \n",
    "        weight_map = np.zeros((height, width), dtype=np.float32)\n",
    "        weight_map[contours > 0] = contours[contours > 0] / counts[contours > 0]\n",
    "        weight_map[weights > 0] = weights[weights > 0]\n",
    "        weight_map[weight_map == 0] = 1\n",
    "\n",
    "        # add zero weights at border and undeclared parcels\n",
    "        weight_map[not_declared == True] = 0\n",
    "        weight_map[:1, :] = 0\n",
    "        weight_map[:, :1] = 0\n",
    "        weight_map[-2:, :] = 0\n",
    "        weight_map[:, -2:] = 0\n",
    "\n",
    "        eopatch[self.weight_feature] = weight_map[..., np.newaxis]\n",
    "        \n",
    "        return eopatch\n",
    "\n",
    "class PredictPatchTask(EOTask):\n",
    "    \"\"\"\n",
    "    https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#6.-Model-construction-and-training\n",
    "    Task to make model predictions on a patch. Provide the model \n",
    "    \"\"\"\n",
    "    def __init__(self, model, features_feature, args):\n",
    "        self.model = model\n",
    "        self.features_feature = features_feature\n",
    "        self.args = args\n",
    "    \n",
    "    def execute(self, eopatch):\n",
    "        pred_eopatch = EOPatch(bbox=eopatch.bbox)\n",
    "        band_names = ['B01','B02','B03','B04','B05','B06','B07','B08','B8A','B09','B11','B12']\n",
    "        tidx = list(range((self.args.n_time_frames+1)//2)) + list(range(-1*(self.args.n_time_frames//2), 0))\n",
    "        print(f'selecting the first N//2 and the last N//2 time stamps: {tidx}')\n",
    "\n",
    "        print(self.args.indices)\n",
    "\n",
    "        x = []\n",
    "        for ix in tidx: # outer most group: time index\n",
    "            print(f'Time index {ix}')\n",
    "            for band in self.args.bands:\n",
    "                band_ix = band_names.index(band)\n",
    "                if len(eopatch.data['BANDS']) > ix:\n",
    "                    xx = eopatch.data['BANDS'][ix][:, :, band_ix]\n",
    "                else:\n",
    "                    xx = eopatch.data['BANDS'][0][:, :, band_ix]\n",
    "                x.append(xx.astype(np.float32))\n",
    "            for index in self.args.indices:\n",
    "                if len(eopatch.data['BANDS']) > ix:\n",
    "                    xx = eopatch.data[index][ix]\n",
    "                else:\n",
    "                    xx = eopatch.data[index][0]\n",
    "                x.append(xx.astype(np.float32).squeeze())\n",
    "                print(f'Normalized index {index} attached')\n",
    "        x = np.expand_dims(np.stack(x), axis=0)\n",
    "        x = torch.tensor(x.astype(np.float32))\n",
    "        print('Input shape: ', x.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model(x)\n",
    "        print('Output shape: ', prediction.shape)\n",
    "        # reshape to expected output shape\n",
    "        prediction = prediction.numpy().squeeze()\n",
    "        prediction = prediction[:, :, np.newaxis]\n",
    "        prediction = np.round(prediction).astype(np.uint8)\n",
    "        pred_eopatch[(FeatureType.MASK_TIMELESS, 'PREDICTION')] = prediction\n",
    "        return pred_eopatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We document our preprocessing pipeline, that was executed for the train and validation dataset prior to training the network. We use 90 EOPatches for training and 10 for validation, randomly selected. The following steps are performed:\n",
    "* Add features NDVI,  NDWI and NDBI\n",
    "* Remove images with cloud cover > 1%\n",
    "* Remove missing data (NaNs)\n",
    "* Add weight maps for MCC calculation\n",
    "\n",
    "The code is found in `./ai4eo/preprocessing.py`. To reproduce our preprocessing, execute\n",
    "\n",
    "`python preprocessing.py --raw-data-dir <yourpath>/eopathes/ --target-dir <yourpath>/dev_data/no_nans_no_clouds/ --cloud-threshold 0.99`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "\n",
    "The next cell contains our version of the SRResNet. It is adapted for our application from https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution and implements the image superresolution network of https://arxiv.org/abs/1609.04802. The hyperparameter optimization was included by us using NNI (https://nni.readthedocs.io/en/stable/). The network hyperparameters that were used for training our final submission model were:\n",
    "- learning rate: 1e-4\n",
    "- batch size: 12\n",
    "- n-channels: 125 \n",
    "- n-blocks: 49 \n",
    "- bands: B02 B03 B04 B05 B06 B07 B08 B8A B11 B12 (all 10m and 20m spatial resolution bands)\n",
    "- n-time-frames: 8 (first 4 frames and last 4 frames)\n",
    "- indices: NDVI NDWI NDBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(nn.Module):\n",
    "    '''\n",
    "    Convolutional block: Convolution, BatchNorm, Activation\n",
    "    credits: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution/blob/master/models.py\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, batch_norm=False, activation=None):\n",
    "        '''\n",
    "        :param in_channels: number of input channels\n",
    "        :param out_channels: number of output channels\n",
    "        :param kernel_size: kernel size\n",
    "        :param stride: stride\n",
    "        :param batch_norm: include a BN layer?\n",
    "        :param activation: Type of activation; None if none\n",
    "        '''\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "\n",
    "        if activation is not None:\n",
    "            activation = activation.lower()\n",
    "            assert activation in {'prelu', 'leakyrelu', 'tanh'}\n",
    "\n",
    "        # container, that will hold the layers in this convolutional block\n",
    "        layers = list()\n",
    "        # convolutional layer\n",
    "        layers.append(\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, \n",
    "                    stride=stride, padding=kernel_size // 2)\n",
    "                )\n",
    "        # batch normalization, if wanted\n",
    "        if batch_norm is True:\n",
    "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
    "\n",
    "        # activation layer, if wanted\n",
    "        if activation == 'prelu':\n",
    "            layers.append(nn.PReLU())\n",
    "        elif activation == 'leakyrelu':\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        elif activation == 'tanh':\n",
    "            layers.append(nn.Tanh())\n",
    "\n",
    "        # put together the convolutional block as a sequence of the layers\n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        '''\n",
    "        Forward propagation\n",
    "\n",
    "        :param input: input images, a tensor of size (N, in_channels, w, h)\n",
    "        :return: output images, a tensor of size (N, out_channels, w, h)\n",
    "        '''\n",
    "        output = self.conv_block(input_) #(N, out_channels, w, h)\n",
    "        return output\n",
    "\n",
    "class SubPixelConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A subpixel convolutional block, comprising convolutional, pixel-shuffle, and PReLU activation layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, scaling_factor=2):\n",
    "        \"\"\"\n",
    "        :param kernel_size: kernel size of the convolution\n",
    "        :param n_channels: number of input and output channels\n",
    "        :param scaling_factor: factor to scale input images by (along both dimensions)\n",
    "        \"\"\"\n",
    "        super(SubPixelConvolutionalBlock, self).__init__()\n",
    "        \n",
    "        # convolutional layer that increases the number of channels by scaling factor^2\n",
    "        # followed by pixel shuffle and PReLU\n",
    "        self.conv = nn.Conv2d(in_channels=args.n_channels, out_channels=args.n_channels * (scaling_factor ** 2),\n",
    "                            kernel_size=args.small_kernel_size, padding=args.small_kernel_size // 2)\n",
    "        \n",
    "        # These additional channels are shuffled to form additional pixels\n",
    "        # upscaling each dimension by the scaling factor\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=scaling_factor)\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param input: input images, a tensor of size (N, n_channels, w, h)\n",
    "        :return: scaled output images, a tensor of size (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "        \"\"\"\n",
    "        output = self.conv(input_)  # (N, n_channels * scaling factor^2, w, h)\n",
    "        output = self.pixel_shuffle(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "        output = self.prelu(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block, comprising two convolutional blocks with a residual connection across them.\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param kernel_size: kernel size\n",
    "        :param n_channels: number of input and output channels (same because the input must be added to the output)\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # first convolutional block\n",
    "        self.conv_block1 = ConvolutionalBlock(in_channels=args.n_channels, out_channels=args.n_channels, \n",
    "                kernel_size=args.small_kernel_size, batch_norm=True, activation='PReLu')\n",
    "\n",
    "        # second convolutional block\n",
    "        self.conv_block2 = ConvolutionalBlock(in_channels=args.n_channels, out_channels=args.n_channels,\n",
    "                kernel_size=args.small_kernel_size, batch_norm=True, activation=None)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        :param input: input images, a tensor of size (N, n_channels, w, h)\n",
    "        :return: output images, a tensor of size (N, n_channels, w, h)\n",
    "        \"\"\"\n",
    "        residual = input_ # (N, n_channels, w, h)\n",
    "        output = self.conv_block1(input_) # (N, n_channels, w, h)\n",
    "        output = self.conv_block2(output) # (N, n_channels, w, h)\n",
    "        output = output + residual # (N, n_channels, w, h)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "class SRResNet(nn.Module):\n",
    "    '''\n",
    "    The SRResNet, as defined in the paper.\n",
    "    credits: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution/blob/master/models.py\n",
    "    '''\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param large_kernel_size: kernel size of the first and last convolutions which transform the inputs and outputs\n",
    "        :param small_kernel_size: kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n",
    "        :param n_channels: number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n",
    "        :param n_blocks: number of residual blocks\n",
    "        :param scaling_factor: factor to scale input images by (along both dimensions) in the subpixel convolutional block\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        super(SRResNet, self).__init__()\n",
    "\n",
    "        # Scaling factor must be 2, 4 or 8\n",
    "        scaling_factor = int(args.scaling_factor)\n",
    "        assert scaling_factor in {2, 4, 8}, \"The scaling factor must be 2, 4, or 8!\"\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv_block1 = ConvolutionalBlock(in_channels=args.input_channels, out_channels=args.n_channels, \n",
    "                kernel_size=args.large_kernel_size,\n",
    "                batch_norm=False, activation='PReLu')\n",
    "\n",
    "        # Sequence of residual blocks\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "                *[ResidualBlock(args) for i in range(args.n_blocks)]\n",
    "                )\n",
    "\n",
    "        # Another convolutional block\n",
    "        self.conv_block2 = ConvolutionalBlock(in_channels=args.n_channels, out_channels=args.n_channels,\n",
    "                kernel_size=args.small_kernel_size, batch_norm=True, activation=None)\n",
    "\n",
    "        # Upscaling: by sub-pixel convolution, each such block upscaling by a factor of 2\n",
    "        n_subpixel_convolutional_blocks = int(math.log2(args.scaling_factor))\n",
    "        print(f'times subpixel: {n_subpixel_convolutional_blocks}')\n",
    "        self.subpixel_convolutional_blocks = nn.Sequential(\n",
    "                *[SubPixelConvolutionalBlock(args, scaling_factor=2) for i in range(n_subpixel_convolutional_blocks)]\n",
    "                )\n",
    "\n",
    "        # Last convolutional block\n",
    "        self.conv_block3 = ConvolutionalBlock(in_channels=args.n_channels, out_channels=1,\n",
    "                kernel_size=args.large_kernel_size, batch_norm=False, activation='Tanh')\n",
    "\n",
    "        # Final sigmoid layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, lr_imgs):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "\n",
    "        :param lr_imgs: low-resolution input images, a tensor of size (N, 3, w, h)\n",
    "        :return: super-resolution output images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n",
    "        \"\"\"\n",
    "        output = self.conv_block1(lr_imgs) # (N, input_channels, w, h)\n",
    "        residual = output # (N, n_channels, w, h)\n",
    "        output = self.residual_blocks(output) # (N, n_channels, w, h)\n",
    "        output =self.conv_block2(output) # (N, n_channels, w, h)\n",
    "        output = output + residual\n",
    "        output = self.subpixel_convolutional_blocks(output) # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "        sr_imgs = self.conv_block3(output) # (N, 1, w * scaling factor, h * scaling factor)\n",
    "        sr_imgs = self.sigmoid(sr_imgs)\n",
    "        return sr_imgs\n",
    "\n",
    "    def get_device(self):\n",
    "        \"\"\"Return gpu if available, else cpu\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return 'cuda:0'\n",
    "        else:\n",
    "            return 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Details:**\n",
    "\n",
    "We trained the model on a single NVIDIA A100 GPU, using 1000 random cropped images of 32 x 32 pixel dimension. Image augmentation is applied, each image is rotated by 90, 180, and 270 degrees in addition to the standard orientation. One epoch of training takes about 50 minutes. We trained for 100 epochs. This resulted in a weighted **MCC score of 0.831**.\n",
    "\n",
    "The code is found in `./ai4eo/model.py`. To reproduce our model training, execute\n",
    "\n",
    "`python model.py --processed-data-dir <yourpath>/dev_data/no_nans_no_clouds/ --s2-random --n-s2 1000 --max-epochs 101 --learning-rate 1e-4 --batch-size 12 --n-channels 125 --n-blocks 49 --bands B02 B03 B04 B05 B06 B07 B08 B8A B11 B12  --n-time-frames 8 --checkpoint-epoch 20 --indices NDVI NDWI NDBI --patience 20 --target-dir <yourpath>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions and create Submission\n",
    "\n",
    "In the next cells the trained model is loaded and predictions are made. A submission is created by performing the necessary preprocessing steps and applying the model. The created images are exported as tiff files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the submission, given the saved model and the following paramters. The only argument that should be adapted is\n",
    "\n",
    "`args.raw_data_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.fixed_random_seed = True\n",
    "args.trained_model = \"best_model.pt\"\n",
    "args.target_dir = 'submission'\n",
    "args.n_processes = 1 # Parallel processes for EOExecutor, on our GPU only 1 was possible\n",
    "args.overwrite = True # overwrite the output files\n",
    "args.flag = 'test' # Make predictions on the test set\n",
    "args.cloud_threshold = 0.999 # Filter all cloudy scenes exceeding 0.1% of cloud coverage\n",
    "\n",
    "args.n_time_frames = 8 # number of time frames\n",
    "args.s2_length = 500 # low res image side length\n",
    "args.batch_size = 1 # make predictions one at a time\n",
    "args.scaling_factor = 4 # low res to high res scaling factor\n",
    "args.n_channels = 125 # SRResNet channels\n",
    "args.bands = ['B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B11', 'B12'] # Sentinel bands\n",
    "args.indices = [\"NDVI\", \"NDWI\", \"NDBI\"]\n",
    "args.large_kernel_size = 9\n",
    "args.small_kernel_size = 3\n",
    "args.n_blocks = 49\n",
    "args.learning_rate = 1e-3 # parameter not used in eval mode, but it needs to be present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our submission function expects a directory structure\n",
    "# <args.raw_datadir>/test/eopatch-00\n",
    "# etc for the test set eopatches\n",
    "args.raw_data_dir = '/test_data/eopatches/' # change this to your local test patch path. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the provided model that is already trained. On CPU, making one prediction took about 2:30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model_state = torch.load(args.trained_model)\n",
    "else:\n",
    "    model_state = torch.load(args.trained_model, map_location=torch.device('cpu'))\n",
    "        \n",
    "args.input_channels = args.n_time_frames*(len(args.bands)+len(args.indices))\n",
    "eomodel = SRResNet(args)\n",
    "eomodel.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(args, eomodel):\n",
    "\n",
    "    # --------------------\n",
    "    # INPUT CHECK\n",
    "    # --------------------\n",
    "\n",
    "    # for compatibility with the args namespace of the SRResNet, we provide\n",
    "    # the default arguments here\n",
    "    assert args.s2_length==500\n",
    "    assert args.batch_size==1\n",
    "\n",
    "    # --------------------\n",
    "    # USEFUL DEFINITIONS\n",
    "    # --------------------\n",
    "\n",
    "    # band specification\n",
    "    # https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/resolutions/spatial\n",
    "    band_names = ['B01','B02','B03','B04','B05','B06','B07','B08','B8A','B09','B11','B12'] # from starter notebook\n",
    "    band_wavelength = [443, 490, 560, 665, 705, 740, 783, 842, 865, 940, 1610, 2190] # nm\n",
    "    band_spatial_resolution = [60, 10, 10, 10, 20, 20, 20, 10, 20, 60, 20, 20] # m\n",
    "\n",
    "    # EOPatches for input\n",
    "    eops_train = os.listdir(args.raw_data_dir)\n",
    "\n",
    "    # --------------------\n",
    "    # DEFINE THE WORKFLOW\n",
    "    # --------------------\n",
    "\n",
    "    # load eopatches from raw data\n",
    "    load_task = LoadTask(path=args.raw_data_dir)\n",
    "\n",
    "    # compute reflectances\n",
    "    compute_reflectances = ComputeReflectances((FeatureType.DATA, 'BANDS'))\n",
    "\n",
    "    # add the features: NDVI, NDWI, NDBI \n",
    "    ndvi = NormalizedDifferenceIndexTask((FeatureType.DATA, 'BANDS'), (FeatureType.DATA, 'NDVI'),\n",
    "                                     [band_names.index('B08'), band_names.index('B04')])\n",
    "    ndwi = NormalizedDifferenceIndexTask((FeatureType.DATA, 'BANDS'), (FeatureType.DATA, 'NDWI'),\n",
    "                                     [band_names.index('B03'), band_names.index('B08')])\n",
    "    ndbi = NormalizedDifferenceIndexTask((FeatureType.DATA, 'BANDS'), (FeatureType.DATA, 'NDBI'),\n",
    "                                     [band_names.index('B11'), band_names.index('B08')])\n",
    "\n",
    "    # validity masks\n",
    "    # Validate pixels using SentinelHub's cloud detection mask and region of acquisition\n",
    "    add_sh_validmask = AddValidDataMaskTask(SentinelHubValidData(), 'IS_VALID')\n",
    "\n",
    "    # Count the number of valid observations per pixel using valid data mask\n",
    "    add_valid_count = AddValidCountTask('IS_VALID', 'VALID_COUNT')\n",
    "\n",
    "    # Filter out cloudy scenes\n",
    "    valid_data_predicate = ValidDataFractionPredicate(args.cloud_threshold)\n",
    "    filter_task = SimpleFilterTask((FeatureType.MASK, 'IS_VALID'), valid_data_predicate)\n",
    "\n",
    "    # Filter out nans\n",
    "    nan_data_predicate = NanDataPredicate()\n",
    "    nan_filter_task = SimpleFilterTask((FeatureType.DATA, 'NDVI'), nan_data_predicate)\n",
    "\n",
    "    # Predict\n",
    "    predict_task = PredictPatchTask(eomodel, (FeatureType.DATA, 'BANDS'), args)\n",
    "        \n",
    "    # EXPORT PREDICTION AS TIFF - copied from starter notebook\n",
    "    # Export the predicted binary mask as tiff for submission\n",
    "    # NOTE: make sure both 0s and 1s are correctly exported\n",
    "    export_task = ExportToTiff(feature=(FeatureType.MASK_TIMELESS, 'PREDICTION'),\n",
    "                          folder=args.target_dir, crs='epsg:32633', image_dtype=np.uint8, no_data_value=255)\n",
    "\n",
    "    save_task = SaveTask(path=args.target_dir, overwrite_permission=True)\n",
    "\n",
    "    # construct the graph\n",
    "    workflow = LinearWorkflow(load_task,\n",
    "                              compute_reflectances,\n",
    "                              ndvi,\n",
    "                              ndwi,\n",
    "                              ndbi,\n",
    "                              add_sh_validmask,\n",
    "                              add_valid_count,\n",
    "                              filter_task,\n",
    "                              nan_filter_task,\n",
    "                              predict_task,\n",
    "                              #save_task,\n",
    "                              export_task\n",
    "                              )\n",
    "\n",
    "    # --------------------\n",
    "    # EXECUTE THE WORKFLOW\n",
    "    # --------------------\n",
    "\n",
    "    # Define additional parameters of the workflow\n",
    "    execution_args = []\n",
    "\n",
    "    # need to specify the files that should be loaded \n",
    "    # loop all available eopatches and add each to the argument list\n",
    "    execution_args = []\n",
    "\n",
    "    eops_test = sorted(os.listdir(f'{args.raw_data_dir}/{args.flag}/'))\n",
    "\n",
    "    for eop_test in eops_test:\n",
    "        eop_exec_args = {\n",
    "                load_task:   {'eopatch_folder': f'{args.flag}/{eop_test}'},\n",
    "                save_task:   {'eopatch_folder': f'highres_{eop_test}'},\n",
    "                export_task: {'filename': f'{eop_test}.tif'}\n",
    "                        }\n",
    "                                \n",
    "        execution_args.append(eop_exec_args)\n",
    "\n",
    "    # Execute the workflow\n",
    "    executor = EOExecutor(workflow, execution_args, save_logs=True)\n",
    "    executor.run(workers=args.n_processes, multiprocess=True)\n",
    "\n",
    "    executor.make_report()\n",
    "    print('Report was saved to location: {}'.format(executor.get_report_filename()))\n",
    "\n",
    "    failed_ids = executor.get_failed_executions()\n",
    "    if failed_ids:\n",
    "        raise RuntimeError(f'Execution failed EOPatches with IDs:\\n{failed_ids}\\n'\n",
    "                           f'For more info check report at {executor.get_report_filename()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this cell to make the predictions on the test set (This can take some time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(args, eomodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./submission') # list the TIFFs that were created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the predictions by comparing RGB images and predicted high resolution maps of cultivated land. *Left column:* earliest cloud-free scene in the growing season. *Center column:* latest cloud-free scene in the growing season. *Right column:* Predicted high resolution map of cultivated land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_ = len(os.listdir(args.target_dir))\n",
    "c_ = 3\n",
    "\n",
    "eops_test = sorted(os.listdir(f'{args.raw_data_dir}/{args.flag}/'))\n",
    "tiff_test = sorted(os.listdir(args.target_dir))\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(4*c_, 4*r_), nrows=r_, ncols=c_)\n",
    "\n",
    "for i, tp, tt in zip(range(r_), eops_test, tiff_test):\n",
    "    eopatch = EOPatch.load(os.path.join(f'{args.raw_data_dir}/{args.flag}/', tp))\n",
    "    \n",
    "    vis_factor = 3.5\n",
    "\n",
    "    norm_factor = eopatch.scalar['NORM_FACTORS'][0]\n",
    "\n",
    "    \n",
    "    valid_data = np.mean(eopatch.mask['IS_DATA'] & ~eopatch.mask['CLM'], axis=(1,2,3))\n",
    "    \n",
    "    tidx = np.where(valid_data>args.cloud_threshold)[0][0] # earliest cloud free scene in growing season\n",
    "    \n",
    "    axs[i,0].imshow(vis_factor * norm_factor * eopatch.data['BANDS'][tidx][..., [3, 2, 1]])\n",
    "    axs[i,0].set_title(f'S2 L2A - {eopatch.timestamp[tidx]}')\n",
    "    \n",
    "    tidx = np.where(valid_data>args.cloud_threshold)[0][-1] # latest cloud free scene in growing season\n",
    "    \n",
    "    axs[i,1].imshow(vis_factor * norm_factor * eopatch.data['BANDS'][tidx][..., [3, 2, 1]])\n",
    "    axs[i,1].set_title(f'S2 L2A - {eopatch.timestamp[tidx]}')\n",
    "    \n",
    "    try:\n",
    "        img = Image.open(os.path.join(args.target_dir, tt))\n",
    "        x_img = np.array(img)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    axs[i,2].imshow(x_img)\n",
    "    axs[i,2].set_title('Predicted map')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
